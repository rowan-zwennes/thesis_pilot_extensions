# Guide to Reproducing Experiments

This appendix provides a step-by-step guide for executing the full experimental pipeline presented in this paper. The process involves configuring environments on both a host machine and a virtual machine, preparing the codebase, processing data, and running benchmark scripts in both Python and R.

## 1. Prerequisites and Initial Environment Setup

Before running the experiments, ensure the following environments are correctly configured.

### On the Virtual Machine (Debian/Ubuntu)

- Ensure the VM is fully configured according to the setup guide.
- Install a recent version of R.
- Install Java, which is required by the `RWeka` package used in the `M5.R` script. Note the installation path for Java, as it may be needed later.
- Install all required R packages for the `Gather_datasets.R` and `M5.R` scripts. The necessary packages are listed in comments at the beginning of each R script.

### On the Host Computer (Any OS)

- A local Python installation is required to execute a single data acquisition script. Any recent version of Python 3 is sufficient.
- Install the necessary Python packages using pip:
  ```bash
  pip install ucimlrepo pandas
  ```

## 2. Codebase Overview and Modifications

The repository you have cloned (`thesis-pilot-extensions`) is a self-contained project with all necessary code modifications already included. The manual steps of copying and replacing files from the original guide are no longer necessary.

This repository builds upon the original PILOT framework by modifying core files and adding a new analysis pipeline. Below is a summary of the key changes.

### Summary of Changes

#### Modified Core Files
The following files from the original PILOT project have been adjusted or extended to support the new experiments:
- `PILOT.py`
- `Tree.py`
- `download_data.py`
- `benchmark_config.py`

#### New Files
The following new files have been added to implement the new benchmarking and analysis workflow:
- `benchmark_new.py`
- `benchmark_util_new.py`

**Note:** The original `benchmark.py` and `benchmark_util.py` files may still be present in the repository for reference but are not used by the new workflow.

## 3. Data Acquisition and Preparation

This phase involves downloading the raw datasets and processing them into a consistent format using both Python and R scripts.

1. **Download Raw Data:** Manually download all datasets from the sources provided in the data overview table.
2. **Execute Python Script on Host:** On your host computer, run the `Thermography.py` script to acquire its specific dataset.
3. **Configure R Script:** Open the `Gather_datasets.R` script in a text editor. Modify the file paths within the script to point to the locations where you downloaded the datasets in the previous steps.
4. **Execute R Script:** Run the `Gather_datasets.R` script. This will process the raw files and generate a standardized set of `.csv` files.
5. **Transfer Data to VM:** Create a new directory named `Data_folder_thesis` inside the `thesis_pilot_extensions/` project directory on your virtual machine. Manually transfer all `.csv` files generated by the R script into this new folder.
6. **Execute Python Preprocessing:** In the virtual machine's terminal, from within the `thesis_pilot_extensions/` directory, execute the following command. This script will load the `.csv` files and save them as preprocessed `.pkl` files for efficient loading during the main experiment.
   ```bash
   python download_data.py
   ```

## 4. Running the Python Benchmarks

The main experiments are executed using the `benchmark_new.py` script. This script runs the specified models on the specified datasets and saves all results.

The script is executed from the terminal as follows:
```bash
python benchmark_new.py -e <experiment_name> [options]
```

### 4.1. Script Arguments and Identifiers

The script's behavior can be controlled with the following arguments.

**Required Argument:**
- `-e <experiment_name>`: Assign a unique name for the experiment run (no spaces). All output will be saved in a directory with this name.

**Optional Arguments:**
- `-m <model_list>`: Specify a comma-separated list of models to run (e.g., `-m PILOT,rf,xgb`). The list is case-insensitive with no spaces. If omitted, all models will be run. The valid model identifiers are provided in the table below.
- `-d <dataset_list>`: Specify a comma-separated list of datasets to use (e.g., `-d admission,airfoil`). The list is case-sensitive with no spaces. If omitted, all datasets will be used. The valid dataset identifiers are listed below.

The valid identifiers for the `-d` flag are:
```
boston, admission, airfoil, communities, ozone, real, 
thermF, slumpFL, rescosts, music, tecator
```

### Model Identifiers

| **Identifier** | **Description** |
|---|---|
| **Baseline and Standard Models** | |
| `CART`, `PILOT`, `RF`, `RAFFLE`, `XGB` | Standard benchmark models. |
| `Ridge`, `Lasso` | Global regularized linear models. |
| `Ridge-PILOT_ensemble`, `...` | Two-stage ensemble extensions. |
| **Multivariate-Guided Splitting Extensions** | |
| `PILOT-Finalist-S_...` | Models with Lasso integrated into the splitting process. |
| `PILOT-Finalist-D_...` | |
| `PILOT-Per-Feature_...` | |
| `PILOT-Full-Multi_...` | |
| **Node-Local Feature Selection (NLFS) Variants** | |
| `PILOT-NLFS_prefix...` | NLFS model (prefix variant). |
| `PILOT-NLFS_LARS...` | NLFS model (LARS variant). |
| `PILOT-NLFS_fallback...` | NLFS model (only fallback strategy used). |

**Naming Conventions:**
- **For PILOT:** The `_df` suffix means that fixed, default hyperparameters are used.
- **For Multivariate-Guided Splitting:** Identifiers with a `_df` suffix use fixed, default hyperparameters. Identifiers *without* the `_df` suffix represent the version where hyperparameters are tuned via cross-validation. All models in this group also have a `_LARS` or `_prefix` suffix specifying the Lasso solver algorithm.
- **For NLFS Variants:** The variants with a `_tuning` suffix are variants of the models where NLFS is only used for hyperparameter tuning. For final model training and prediction, normal PILOT is used.

**Important Notes:** Running all models on all datasets is computationally intensive and requires significant RAM and time. It is recommended to run experiments in smaller, targeted batches. Parameters for visualizations (e.g., figure size) and feature importance calculations must be adjusted manually within the `benchmark_new.py` and `benchmark_config.py` files before execution.

## 5. Running the M5 Benchmark in R

To ensure a fair comparison, the M5 model is run in R using the exact same cross-validation folds generated by the Python benchmark script. This final step requires transferring fold data from the VM back to the machine where R is used.

1.  **Locate Python Output:** After running a Python experiment, navigate to the output directory located inside your project folder at `thesis_pilot_extensions/Output/<experiment_name>/`. This directory contains two crucial subdirectories:
    - `fold_indices/`: Contains `.json` files defining the train/test splits for each fold of each dataset.
    - `real_datasets/`: Contains `.json` files with the specially transformed data for the Real Estate dataset for each fold.

2.  **Transfer Data:** Manually transfer the entire `fold_indices` and `real_datasets` directories from the VM to a location accessible by your R environment.

3.  **Configure and Run M5 Script:**
    - Open the `M5.R` script.
    - Modify the file paths within the script to point to the locations of the transferred `fold_indices`, `real_datasets`, and the Java installation path if necessary.
    - Execute the `M5.R` script to generate the M5 model results using the consistent data folds.

## 6. (Optional) Aggregation and Processing of Results

This step involves collating the performance metrics from the various output files to produce the final summary tables. Due to the computational expense of the full experiment, results were often aggregated manually from separate output directories.

For each model on each dataset, locate the corresponding output file and extract the average Mean Squared Error (MSE) across the five test folds. Once these raw MSE values are gathered into an Excel file (with models as rows and datasets as columns), the provided `Main_table_code.R` script is used to automate the final calculations of relative MSEs and ranks. To use the script, update the file path variable at the top to point to the correct input file. This process is used to generate the final tables for the replication set, the total model set, and the re-normalized comparison table for the original paper's data (although for the last a separate Excel file must be created).

## 7. (Optional) Generating the Time Complexity Plot

To create the plot that supports the hypothesis regarding algorithm scalability of NLFS, follow these steps:

1. **Create a CSV file manually** (e.g., in Microsoft Excel) with the following columns: `Complexity`, `PILOT`, `PNLFS_P`, `PNLFS_L`, `PNLFS_F`.

2. **Populate the file:**
   - In the `Complexity` column, enter the complexity metric (N Ã— D) for each dataset used in the time comparison.
   - In each of the model columns, enter the total fitting time (in seconds) for that model on the corresponding dataset, as aggregated in the previous step.

3. **Save the CSV file** and note its location.

4. **Update and Run the R Script:**
   - Open the provided `NLFS_Plot.R` script.
   - At the top of the script, update the `file_path` variable to point to the full path of the CSV file you just created.
   - Ensure all necessary R packages listed at the top of the script are installed.
   - Execute the script to generate the log-log plot of fitting time versus dataset complexity.

## 8. (Optional) Generating the Feature Importance Plot

To create a publication-quality plot that visualizes and compares the stability and average importance of features across different models, follow these steps. This process uses the raw, per-fold importance values to generate a clustered bar chart with error bars, offering a more robust analysis than using pre-averaged data.

1. **Create a CSV file manually** (e.g., in Microsoft Excel) to store the raw importance data in a "long format". This file requires the following columns: `Model`, `Fold`, `Feature`, `Importance`.

2. **Populate the file with raw, per-fold data:**
   - For each model's experimental run, locate the feature importance values generated for each of the 5 cross-validation folds.
   - For every fold, add a row for each feature. For example, the results for a single model over 5 folds with 6 features will require 30 rows.
   - In the `Model` column, enter the model's name (e.g., `PILOT`, `PNLFS_L`).
   - In the `Fold` column, enter the corresponding fold number (1, 2, 3, 4, or 5).
   - In the `Feature` column, enter the feature name (e.g., `X1`, `X2`).
   - In the `Importance` column, enter the normalized importance value for that specific feature in that specific fold.

3. **Generate the Plot from Collated Data:**
   - **Select the Chart Type:** Create a Clustered Column Chart (also known as a grouped bar chart). This is the most effective way to compare the importance of a single feature across all the different models side-by-side.
   - **Assign Data to Axes:**
     - The Horizontal Axis (X-axis) should represent the features (X1, X2, etc.).
     - The Vertical Axis (Y-axis) should represent the average feature importance calculated from the folds.
     - Each Model will be a separate data series, creating the clustered columns for each feature.

## 9. (Optional) Generating the Optimal `max_depth` Frequency Plot

To create the plot comparing the structural complexity favored by each model, follow these steps.

1. **Collate Optimal Hyperparameters:** For each model on each dataset and cross-validation fold, identify the optimal `max_depth` value that resulted from the hyperparameter tuning process.

2. **Create a Frequency Table:** Manually create a table (e.g., in Microsoft Excel) with models as rows and the candidate `max_depth` values (3, 6, 9, 12, 15, 18) as columns. Populate each cell with the frequency (count) of how many times that depth was selected as optimal for that model across all runs.

3. **Generate the Figure:** Using the frequency table, create a **Line with Markers** chart. To improve clarity for publication, it is recommended to create simplified plots that either show a smaller subset of models or represent the average frequencies of related model families. Ensure the final figure is clearly formatted with a title, axis labels (e.g., 'Frequency' and 'Optimal `max_depth`'), and a legend.
